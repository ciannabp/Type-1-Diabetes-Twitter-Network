---
title: "t1d_scraping_public"
author: "Cianna Bedford-Petersen"
date: "last knitted:`r Sys.time()`"
output: 
  html_document:
      df_print: paged
      toc_float: true 
      code_folding: show
---

```{r setup, include=FALSE}

library(rio)
library(tidyverse)
library(tidytext)
library(rtweet)
library(glue)
library(here)


knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
```

# Settings for this particular session 
```{r settings}

# how many tweets to start with 
n_tweets = 1500
# how many followers from each user 
n_follower = 5000
# how many tweets per person
n_timeline = 100

# specify the hashtags
hashtags <-  c("t1d", "t1dlookslikeme", "brokenpancreas", "type1kid", "typeonetypenone", "diabadass", 
               "type1warrior", "beyondtype1","insulindependent", "typeonestrong", "dexcom", "GBdoc")

# set directory for session

date = Sys.Date()
folder = paste0("data_", as.character(date))
dir.create(here(folder))

```

# Keys
```{r keys}
#Insert personal Twitter API keys and tokens
app  = ""
consumer_key <- ''
consumer_secret <- ''
access_token <- ''
access_secret <- ''


token <- create_token(
  app = app, 
  consumer_key = consumer_key, 
  consumer_secret = consumer_secret,
  access_token = access_token, 
  access_secret = access_secret)
```

# First pass at scraping
```{r first scraping}
# scrape tweets
# type mixed means a combination of the most recent tweets and the most popular 
t1d_twitter <- rtweet::search_tweets2(paste0("#", hashtags, collapse = " OR "), 
                                    n = n_tweets, 
                                    env_name = "research") 

first_pass_nrow = nrow(t1d_twitter)
first_pass_ntweets = length(unique(t1d_twitter$text))
first_pass_nusers = length(unique(t1d_twitter$user_id))

```

# Remove missing columns
```{r remove missing}
# remove missing values
all.missing <-  function(x){
  num.miss <-  length(which(is.na(x)))
  response <-  ifelse(num.miss == length(x), TRUE, FALSE)
}

whichmiss <-  apply(t1d_twitter, 2, all.missing)
t1d_twitter = t1d_twitter[,-whichmiss]

```

# Function to get timelines
```{r}
#function to get timelines
get_timeline_unlimited <- function(users, n){
  
  if (length(users) ==0){
    return(NULL)
  }
  
  rl <- rate_limit(query = "get_timeline")
  
  if (length(users) <= rl$remaining){
    print(glue::glue("Getting data for {length(users)} users"))
    tweets <- get_timeline(users, n, check = FALSE)  
  }else{
    
    if (rl$remaining > 0){
      users_first <- users[1:rl$remaining]
      users_rest <- users[-(1:rl$remaining)]
      print(glue::glue("Getting data for {length(users_first)} users"))
      tweets_first <- get_timeline(users_first, n, check = FALSE)
      rl <- rate_limit(query = "get_timeline")
    }else{
      tweets_first <- NULL
      users_rest <- users
    }
    wait <- rl$reset + 0.1
    print(glue::glue("Waiting for {round(wait,2)} minutes"))
    Sys.sleep(wait * 60)
    
    tweets_rest <- get_timeline_unlimited(users_rest, n)  
    tweets <- bind_rows(tweets_first, tweets_rest)
  }
  return(tweets)
}

```

#Filter timeline data
```{r filter timelines}
# filter for unique screen names
unique_users <-  unique(t1d_twitter$screen_name)

# to figure how long this code takes to run, divide the number of 
firsttweets <-  get_timeline_unlimited(users = unique_users, n_timeline)

# filter for participants who have tweeted one of the hashtags on three separate days

# for each tweet, were any of the hashtags used?
incommunity <-  str_detect(firsttweets$text, regex(paste(hashtags, collapse = "|"), ignore_case = T))
firsttweets$incommunity <-  as.numeric(incommunity)

firsttweets <-  firsttweets %>%
  #separate date and time stamp into separate parts -- we only care about the datestamp
  separate(created_at, into = c("Date", "Time", "Zone"), sep = " ") %>% 
  #for each date for each user, put 1 if any tweets contain the key words
  group_by(user_id, Date) %>%
  mutate(community = ifelse(sum(incommunity) > 0, 1, 0)) %>%
  ungroup() %>%
  # now for each user, count how many days they use at least one of the key words
  group_by(user_id) %>%
  mutate(num_community = sum(incommunity)) %>%
  ungroup() %>%
  # remove participants with fewer than 3 days using the key words
  filter(num_community > 2)

# export 
save(firsttweets, file = here(paste0(folder, "/firsttweets.Rdata")))

firsttweets_nrow = nrow(firsttweets)
firsttweets_ntweets = length(unqiue(firsttweets$text))
firsttweets_nusers = length(unqiue(firsttweets$user_id))

save(first_pass_nrow, first_pass_ntweets, first_pass_nusers,
     firsttweets_nrow, firsttweets_ntweets, firsttweets_nusers,
     file = here(paste0(folder, "/scraping_info.Rdata")))


```

# Get follower timelines
```{r}
# new set of unique users 
unique_users <-  unique(firsttweets$screen_name)

user_profile = lookup_users(users = unique_users)

# pull followers ------------------------------------

# pull follower data on active users
rm(firsttweets)

for (i in seq_along(unique_users)) {
  message("Getting followers for user #", i, "/", length(unique_users))
  followers <- get_followers(unique_users[i], 
                                  n = n_follower, retryonratelimit = TRUE)

  n_followers = nrow(followers)
  save(n_followers, file = here(paste0(folder, "/nfollowers_", i, ".Rdata")))

  new_ids = unlist(followers)

  followertweets <-  get_timeline_unlimited(users = new_ids, n_timeline)
    # pull follower tweets
    new_ids = followers$user_id
    
    followertweets <-  get_timeline_unlimited(users = new_ids, n_timeline)
    
    if(nrow(followertweets) > 0){
    
    # for each tweet, were any of the hashtags used?
    incommunity <-  str_detect(followertweets$text, regex(paste(hashtags, collapse = "|"), ignore_case = T))
    followertweets$incommunity <-  as.numeric(incommunity)
    
    followertweets <-  followertweets %>%
      #separate date and time stamp into separate parts -- we only care about the datestamp
      separate(created_at, into = c("Date", "Time", "Zone"), sep = " ") %>% 
      #for each date for each user, put 1 if any tweets contain the key words
      group_by(user_id, Date) %>%
      mutate(community = ifelse(sum(incommunity) > 0, 1, 0)) %>%
      ungroup() %>%
      # now for each user, count how many days they use at least one of the key words
      group_by(user_id) %>%
      mutate(num_community = sum(incommunity)) %>%
      ungroup() %>%
      # remove participants with fewer than 3 days using the key words
      filter(num_community > 2)}
    
    if(nrow(followertweets) > 0) {save(followertweets, file = here(paste0(folder, "/followertweets_", i, ".Rdata")))
      }
}
```

